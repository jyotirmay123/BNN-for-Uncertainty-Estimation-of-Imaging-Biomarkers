{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import nibabel as nb\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/home/abhijit/Jyotirmay/my_thesis/projects/MC_dropout_quicknat/reports/MC_dropout_quicknat_UKB_v2/UKB/10_0.0_report.csv', index_col=0)\n",
    "#df['real_volume_id'] = df['volume_id']\n",
    "#df['volume_id'] = df['volume_id']+df['target_scan_file'].apply(lambda x:x.split('.')[0][-1])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('/home/abhijit/Jyotirmay/my_thesis/projects/MC_dropout_quicknat/reports/MC_dropout_quicknat_UKB_v2/UKB/0_0.0_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_counts(model_props):   \n",
    "    sample_count_dict = {}\n",
    "    base_dir, proj, model, dataset, report_id = model_props\n",
    "    if dataset=='KORA':\n",
    "        sample_dir = f'{base_dir}/projects/{proj}/outs/{proj}_{dataset}_{model}/{proj}_{dataset}_{model}_predictions_{dataset}/*_samples/**'\n",
    "        mean_segs_dir = f'{base_dir}/projects/{proj}/outs/{proj}_{dataset}_{model}/{proj}_{dataset}_{model}_predictions_{dataset}/*_seg.nii.gz'\n",
    "    else:\n",
    "        ukb_out_dir = '/home/abhijit/nas_drive/Abhijit/Jyotirmay/ukb_outs'\n",
    "        #'/home/abhijit/nas_drive/Abhijit/Jyotirmay/ukb_outs/outs/probabilistic_quicknat_UKB_v2/\n",
    "        #probabilistic_quicknat_UKB_v2_predictions_UKB'\n",
    "        sample_dir = f'{ukb_out_dir}/outs/{proj}_{dataset}_{model}/{proj}_{dataset}_{model}_predictions_{dataset}/*_samples/**'\n",
    "        mean_segs_dir = f'{ukb_out_dir}/outs/{proj}_{dataset}_{model}/{proj}_{dataset}_{model}_predictions_{dataset}/*_seg.nii.gz'\n",
    "    \n",
    "    seg_paths = glob.glob(sample_dir)[:25000]\n",
    "    mean_seg_paths = glob.glob(mean_segs_dir)[:2500]\n",
    "    seg_paths.extend(mean_seg_paths)\n",
    "    for r_id, sample in enumerate(seg_paths):\n",
    "        print(\"\\rProcessing {} {}/{}.\".format(proj, r_id, len(seg_paths)), end=\"\")\n",
    "\n",
    "        split_str = sample.split('/')[-1].split('.')[0]\n",
    "        v_id = split_str.split('_')[0]\n",
    "        if dataset == 'UKB':\n",
    "            v_id = v_id + '_20201_2_0'\n",
    "            #if v_id not in good_vols:\n",
    "            #    continue\n",
    "        sample_id = split_str.split('_')[-1]\n",
    "        nifti_file = nb.load(sample)\n",
    "        header = nifti_file.header\n",
    "        pix_volume = np.prod([2,2,3])\n",
    "        s_data = nifti_file.get_fdata()\n",
    "        unique_vals, counts = np.unique(s_data, return_counts=True)\n",
    "        if len(counts) is not 3:\n",
    "            print('not all class present, skipping!!')\n",
    "            continue\n",
    "        if v_id not in sample_count_dict.keys():\n",
    "            sample_count_dict[v_id] = {str(i)+'_spleen':None for i in range(10)}\n",
    "            sample_count_dict[v_id].update({str(i)+'_liver':None for i in range(10)})\n",
    "        if sample_id == 'seg':\n",
    "            sample_count_dict[v_id]['seg_liver'] = np.round(counts[2]*pix_volume)\n",
    "            sample_count_dict[v_id]['seg_spleen'] = np.round(counts[1]*pix_volume)\n",
    "        else:\n",
    "            sample_count_dict[v_id][str(sample_id)+'_liver'] = counts[2]*pix_volume    \n",
    "            sample_count_dict[v_id][str(sample_id)+'_spleen'] = counts[1]*pix_volume\n",
    "    \n",
    "    print('')\n",
    "    df = pd.DataFrame.from_dict(sample_count_dict, orient=\"index\")\n",
    "    path_to_save = f'{base_dir}/projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_sample_count_report.csv'\n",
    "    df.to_csv(path_to_save, index_label='volume_id')\n",
    "    return {f'{proj}': path_to_save}\n",
    "\n",
    "def ground_truth_vol_counts(dataset_props, vols_to_look=None, vols_to_look_type='all'):\n",
    "    sample_count_dict = {}\n",
    "    base_dir, glob_dir, dataset_group, dataset, process_status = dataset_props\n",
    "    if process_status:\n",
    "        seg_paths = glob.glob(f'{base_dir}/{dataset_group}/{dataset}/processed_data/labels/**')\n",
    "    else:\n",
    "        seg_paths = glob.glob(glob_dir)\n",
    "    for r_id, sample in enumerate(seg_paths):\n",
    "        print(\"\\rProcessing {} {} {}/{}.\".format(vols_to_look_type, dataset, r_id, len(seg_paths)), end=\"\")\n",
    "        nifti_file = nb.load(sample)\n",
    "        header = nifti_file.header\n",
    "        if process_status:\n",
    "            v_id = sample.split('/')[-1].split('.')[0]\n",
    "            pix_volume = np.prod([2,2,3])\n",
    "        else:\n",
    "            v_id = sample.split('/')[-2]\n",
    "            pix_volume = np.prod(header['pixdim'][1:4])\n",
    "        \n",
    "        if vols_to_look is not None and v_id in vols_to_look:\n",
    "            continue\n",
    "        \n",
    "        s_data = nifti_file.get_fdata()\n",
    "        \n",
    "        unique_vals, counts = np.unique(s_data, return_counts=True)\n",
    "        if len(counts) is not 3:\n",
    "            print('not all class present, skipping!!')\n",
    "            continue\n",
    "        if v_id not in sample_count_dict.keys():\n",
    "            sample_count_dict[v_id] = {}\n",
    "\n",
    "        sample_count_dict[v_id]['seg_liver'] = np.round(counts[2]*pix_volume)\n",
    "        sample_count_dict[v_id]['seg_spleen'] = np.round(counts[1]*pix_volume)\n",
    "    \n",
    "    print('')\n",
    "    df = pd.DataFrame.from_dict(sample_count_dict, orient=\"index\")\n",
    "    path_to_save = f'{base_dir}/{dataset_group}/{dataset}/{vols_to_look_type}_processed_{process_status}_sample_count_report.csv'\n",
    "    df.to_csv(path_to_save, index_label='volume_id')\n",
    "    return {f'{vols_to_look_type}_{dataset}_processed_{process_status}': path_to_save}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_target_and_volumeid(df_path='/home/abhijit/Jyotirmay/my_thesis/projects/MC_dropout_quicknat/reports/MC_dropout_quicknat_UKB_v2/UKB/10_1573078374.453554_report.csv'):\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "    if df['dataset'].values[1] != 2:\n",
    "        print('Not a UKB dataset report, not processing!')\n",
    "        return False\n",
    "    def replace_func(value):\n",
    "        string = value[-1]\n",
    "        return f'Dixon_BH_17s_opp_Dixon_BH_17{string}.nii.gz' if string == 's' else f'Dixon_BH_17s_opp_Dixon_BH_17s{string}.nii.gz'\n",
    "    df['target_scan_file'] = df['volume_id'].apply(replace_func)\n",
    "    df['volume_id'] = df['volume_id'].apply(lambda x: x[:-1])\n",
    "    return df\n",
    "\n",
    "def concat_partial_model_reports(paths):\n",
    "    df = None\n",
    "#     ids = ['10_1573078374.453554', '10_1573225388.879571', '10_1573391294.0298784']\n",
    "#     paths = [f'/home/abhijit/Jyotirmay/my_thesis/projects/MC_dropout_quicknat/reports/MC_dropout_quicknat_UKB_v2/UKB/{i}_report.csv' for i in ids]\n",
    "    \n",
    "    for ix, p in enumerate(paths):\n",
    "        df_ = organise_target_and_volumeid(p)\n",
    "        df_ = df_.drop(df_.index[0])\n",
    "        if ix == 0:\n",
    "            df = df_\n",
    "        else:\n",
    "            df = df.append(df_, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_model_features(props):\n",
    "    base_dir, proj, model, dataset, report_id = props\n",
    "    print(props)\n",
    "    df = pd.read_csv(f'./projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_sample_count_report.csv')\n",
    "    if dataset is 'KORA':\n",
    "        df_k_feats = pd.read_csv('./dataset_groups/whole_body_datasets/KORA/processsed_csv_.csv')\n",
    "        df_reports = pd.read_csv(f'./projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_report.csv')\n",
    "    else:\n",
    "        df_k_feats = pd.read_csv('./dataset_groups/whole_body_datasets/UKB/ukb_diabetes_basic_feats.csv')\n",
    "        df_reports = pd.read_csv(f'./projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_report.csv')\n",
    "\n",
    "#         df_reports = organise_target_and_volumeid(f'./projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_report.csv')\n",
    "    \n",
    "    df['volume_id'] = df['volume_id'].astype(str)\n",
    "    df_k_feats['volume_id'] = df_k_feats['volume_id'].astype(str)\n",
    "    df_reports['volume_id'] = df_reports['volume_id'].astype(str)\n",
    "    df_merged = pd.merge(df, df_k_feats, how='inner', on=['volume_id'])\n",
    "    df_merged_final = pd.merge(df_merged, df_reports, how='inner', on=['volume_id'])\n",
    "    path_to_save = f'./projects/{proj}/reports/{proj}_{dataset}_{model}/{dataset}/{report_id}_concat_report_final.csv'\n",
    "    df_merged_final.to_csv(path_to_save)\n",
    "    return {f'{proj}':path_to_save}\n",
    "\n",
    "def merge_all_dataset_features(props, vols_to_look_type='all'):\n",
    "    base_dir, glob_dir, dataset_group, dataset, process_status = props\n",
    "    df = pd.read_csv(f'{base_dir}/{dataset_group}/{dataset}/{vols_to_look_type}_processed_{process_status}_sample_count_report.csv')\n",
    "    if dataset is 'KORA':\n",
    "        df_k_feats = pd.read_csv('./dataset_groups/whole_body_datasets/KORA/processsed_csv_.csv')\n",
    "    else:\n",
    "        df_k_feats = pd.read_csv('./dataset_groups/whole_body_datasets/UKB/ukb_diabetes_basic_feats.csv')\n",
    "    df_merged = pd.merge(df, df_k_feats, how='inner', on=['volume_id'])\n",
    "    path_to_save = f'{base_dir}/{dataset_group}/{dataset}/{vols_to_look_type}_processed_{process_status}_concat_report_final.csv'\n",
    "    df_merged.to_csv(path_to_save)\n",
    "    return {f'{vols_to_look_type}_{dataset}_processed_{process_status}':path_to_save}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Segmentation data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis/dataset_groups'\n",
    "glob_dir = '/home/abhijit/nas_drive/Data_WholeBody/KORA/KORA_segs/ROI_liver_spleen_Daniel/**/comp_mask.nii'\n",
    "dataset_group = 'whole_body_datasets'\n",
    "dataset = 'KORA'\n",
    "process_status = False\n",
    "\n",
    "unprocessed_dataset_props = base_dir, glob_dir, dataset_group, dataset, process_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis/dataset_groups'\n",
    "glob_dir = '/home/abhijit/Jyotirmay/my_thesis/dataset_groups'\n",
    "dataset_group = 'whole_body_datasets'\n",
    "dataset = 'KORA'\n",
    "process_status = True\n",
    "\n",
    "processed_dataset_props_kora = base_dir, glob_dir, dataset_group, dataset, process_status\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis/dataset_groups'\n",
    "glob_dir = '/home/abhijit/Jyotirmay/my_thesis/dataset_groups'\n",
    "dataset_group = 'whole_body_datasets'\n",
    "dataset = 'UKB'\n",
    "process_status = True\n",
    "\n",
    "processed_dataset_props_ukb = base_dir, glob_dir, dataset_group, dataset, process_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full_bayesian Segmentation data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'full_bayesian'\n",
    "model = 'v2'\n",
    "dataset = 'KORA'\n",
    "report_id = '10_1571866968.4002764'\n",
    "\n",
    "fb_props_kora = base_dir, proj, model, dataset, report_id\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'full_bayesian'\n",
    "model = 'v2'\n",
    "dataset = 'UKB'\n",
    "report_id = ''\n",
    "\n",
    "fb_props_ukb = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "# proj = 'full_bayesian'\n",
    "# model = 'v3'\n",
    "# dataset = 'KORA'\n",
    "# report_id = '10_1572536287.589728'\n",
    "\n",
    "# fb_props_0dot05 = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'full_bayesian_0dot01'\n",
    "model = 'v4'\n",
    "dataset = 'KORA'\n",
    "report_id = '10_1572514598.527084'\n",
    "\n",
    "fb_props_0dot01_kora = base_dir, proj, model, dataset, report_id\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'full_bayesian_0dot01'\n",
    "model = 'v4'\n",
    "dataset = 'UKB'\n",
    "report_id = ''\n",
    "\n",
    "fb_props_0dot01_ukb = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC_Dropout Segmentation data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'MC_dropout_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'KORA'\n",
    "report_id = '10_1572006141.7793334'\n",
    "\n",
    "mcdropout_props_kora = base_dir, proj, model, dataset, report_id\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'MC_dropout_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'UKB'\n",
    "report_id = '0_0.0'\n",
    "\n",
    "mcdropout_props_ukb = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probablisitic_quicknat Segmentation data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'probabilistic_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'KORA'\n",
    "report_id = '10_1571996796.7963011'\n",
    "\n",
    "probabilistic_props_kora = base_dir, proj, model, dataset, report_id\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'probabilistic_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'UKB'\n",
    "report_id = '10_1573834823.1121247'\n",
    "\n",
    "probabilistic_props_ukb = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical_quicknat Segmentation data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'hierarchical_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'KORA'\n",
    "report_id = '10_1571905560.9377904'\n",
    "\n",
    "hierarchical_props_kora = base_dir, proj, model, dataset, report_id\n",
    "\n",
    "base_dir = '/home/abhijit/Jyotirmay/my_thesis'\n",
    "proj = 'hierarchical_quicknat'\n",
    "model = 'v2'\n",
    "dataset = 'UKB'\n",
    "report_id = ''\n",
    "\n",
    "hierarchical_props_ukb = base_dir, proj, model, dataset, report_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_prop = [fb_props_ukb, mcdropout_props_ukb, probabilistic_props_ukb, \n",
    "                   hierarchical_props_ukb, fb_props_0dot01_ukb]\n",
    "all_models_prop = [probabilistic_props_ukb]\n",
    "\n",
    "all_datasets_prop = [processed_dataset_props_ukb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kora_test_vols = ['KORA2459774','KORA2453328','KORA2452913','KORA2452353','KORA2456928','KORA2462380','KORA2459873',\n",
    "             'KORA2453082','KORA2455268','KORA2452967','KORA2453048','KORA2453677','KORA2452812','KORA2460903',\n",
    "             'KORA2452364','KORA2460348','KORA2461338','KORA2461868','KORA2460326','KORA2453172','KORA2453136',\n",
    "             'KORA2452206','KORA2460878','KORA2452129','KORA2456278','KORA2456199','KORA2460565','KORA2454788',\n",
    "             'KORA2460174','KORA2453290','KORA2459605','KORA2460768','KORA2460504','KORA2453194','KORA2452834',\n",
    "             'KORA2459123','KORA2453150','KORA2462345','KORA2460830','KORA2459310','KORA2459455','KORA2459763',\n",
    "             'KORA2453620','KORA2461392','KORA2453578','KORA2453524','KORA2452409','KORA2460785','KORA2460867',\n",
    "             'KORA2453642','KORA2459972','KORA2456202','KORA2455946','KORA2456917','KORA2452426','KORA2461184',\n",
    "             'KORA2456379','KORA2456241','KORA2456793','KORA2452263','KORA2460633','KORA2452316','KORA2453844',\n",
    "             'KORA2462093','KORA2459752','KORA2453470','KORA2459477','KORA2453306','KORA2461409','KORA2460779',\n",
    "             'KORA2460309','KORA2456340','KORA2461206','KORA2460824','KORA2461956','KORA2459908','KORA2452924',\n",
    "             'KORA2459947','KORA2453464','KORA2460216','KORA2461493','KORA2453374','KORA2452687','KORA2460315',\n",
    "             'KORA2461146','KORA2460249','KORA2452941','KORA2453732','KORA2452338','KORA2453363','KORA2453833',\n",
    "             'KORA2459526','KORA2459983','KORA2461632','KORA2459548','KORA2462150','KORA2461520','KORA2459807',\n",
    "             'KORA2462374','KORA2458040','KORA2456661','KORA2452659','KORA2458366','KORA2452801','KORA2453811',\n",
    "             'KORA2456672','KORA2457266','KORA2457044','KORA2453765','KORA2458402','KORA2461349','KORA2455525',\n",
    "             'KORA2455296','KORA2458158','KORA2460447','KORA2455753','KORA2455935','KORA2456562','KORA2458197',\n",
    "             'KORA2458707','KORA2455951','KORA2460889','KORA2460472','KORA2456385','KORA2455042','KORA2452665',\n",
    "             'KORA2452190','KORA2462161','KORA2461151','KORA2459681','KORA2458068','KORA2457401','KORA2461555',\n",
    "             'KORA2459745','KORA2453037','KORA2458265','KORA2457517','KORA2452868','KORA2462119','KORA2452381',\n",
    "             'KORA2452094','KORA2459067','KORA2462352','KORA2456357','KORA2459244','KORA2461577','KORA2456021',\n",
    "             'KORA2459499','KORA2461885','KORA2461252','KORA2460145','KORA2453589','KORA2460543'\n",
    "            ]\n",
    "\n",
    "ukb_test_vols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing probabilistic_quicknat 1913/27500."
     ]
    }
   ],
   "source": [
    "model_vols = [vol_counts(prop) for prop in all_models_prop]\n",
    "# dataset_vols = [ground_truth_vol_counts(prop, vols_to_look=None, vols_to_look_type='all') for prop in all_datasets_prop]\n",
    "# dataset_vols = [ground_truth_vol_counts(prop, vols_to_look=ukb_test_vols, vols_to_look_type='test') for prop in all_datasets_prop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merged_feats_path = [merge_all_model_features(prop) for prop in all_models_prop]\n",
    "# all_dataset_merged_feats_path = [merge_all_dataset_features(prop) for prop in all_datasets_prop]\n",
    "# test_dataset_merged_feats_path = [merge_all_dataset_features(prop, vols_to_look_type='test') for prop in all_datasets_prop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_merged_feats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_dataset_merged_feats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset_merged_feats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tenv]",
   "language": "python",
   "name": "conda-env-tenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
